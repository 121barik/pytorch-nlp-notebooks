{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "char_text_generation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRyoQrbvtdLy",
        "colab_type": "text"
      },
      "source": [
        "# char-RNN: Character-level text generation\n",
        "\n",
        "From day-to-day weather patterns to stock market fluctuations, from music to novels; sequential data is everywhere."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5ztGGH--rfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install boltons -q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ_sI6SI-UIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "from pathlib import Path\n",
        "from textwrap import wrap\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchviz import make_dot, make_dot_from_trace\n",
        "from boltons.iterutils import windowed\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXUE8n_v-UIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKw4lpSO-UI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_PATH = 'data/weight_loss/articles.jsonl'\n",
        "if not Path(DATA_PATH).is_file():\n",
        "    gdd.download_file_from_google_drive(\n",
        "        file_id='1mafPreWzE-FyLI0K-MUsXPcnUI0epIcI',\n",
        "        dest_path='data/weight_loss/weight_loss_articles.zip',\n",
        "        unzip=True,\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe1X3_lY-UI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(path, sequence_length=125):\n",
        "    texts = pd.read_json(path).text.sample(100).str.lower().tolist()\n",
        "    chars_windowed = [list(windowed(text, sequence_length)) for text in texts]\n",
        "    all_chars_windowed = [sublst for lst in chars_windowed for sublst in lst]\n",
        "    filtered_good_chars = [\n",
        "        sequence for sequence in tqdm_notebook(all_chars_windowed) \n",
        "        if all(char in string.printable for char in sequence)\n",
        "    ]\n",
        "    return filtered_good_chars\n",
        "\n",
        "\n",
        "def get_unique_chars(sequences):\n",
        "    return {sublst for lst in sequences for sublst in lst}\n",
        "\n",
        "\n",
        "def create_char2idx(sequences):\n",
        "    unique_chars = get_unique_chars(sequences)\n",
        "    return {char: idx for idx, char in enumerate(sorted(unique_chars))}\n",
        "\n",
        "\n",
        "def encode_sequence(sequence, char2idx):\n",
        "    return [char2idx[char] for char in sequence]\n",
        "\n",
        "\n",
        "def encode_sequences(sequences, char2idx):\n",
        "    return np.array([\n",
        "        encode_sequence(sequence, char2idx) \n",
        "        for sequence in tqdm_notebook(sequences)\n",
        "    ])\n",
        "\n",
        "\n",
        "class Sequences(Dataset):\n",
        "    def __init__(self, path, sequence_length=125):\n",
        "        self.sequences = load_data(DATA_PATH, sequence_length=sequence_length)\n",
        "        self.vocab_size = len(get_unique_chars(self.sequences))\n",
        "        self.char2idx = create_char2idx(self.sequences)\n",
        "        self.idx2char = {idx: char for char, idx in self.char2idx.items()}\n",
        "        self.encoded = encode_sequences(self.sequences, self.char2idx)\n",
        "        \n",
        "    def __getitem__(self, i):\n",
        "        return self.encoded[i, :-1], self.encoded[i, 1:]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZxQzaME-UJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = Sequences(DATA_PATH, sequence_length=128)\n",
        "len(dataset)\n",
        "train_loader = DataLoader(dataset, batch_size=4096)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKupaX61za9e",
        "colab_type": "text"
      },
      "source": [
        "## GRU: Gated Recurrent Unit\n",
        "\n",
        "![](images/char-rnn.png)\n",
        "\n",
        "The following function is computed for each element in the input sequence in the GRU cell:\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "    r_t = \\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\n",
        "    z_t = \\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\n",
        "    n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\\\\n",
        "    h_t = (1 - z_t) * n_t + z_t * h_{(t-1)}\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "where $h_t$ is the hidden state at time $t$, $x_t$ is the input at time $t$, $h_{(t-1)}$ is the hidden state of the layer at time $t-1$ or the initial hidden state at time $0$, and $r_t$, $z_t$, $n_t$ are the reset, update, and new gates, respectively. $\\sigma$ is the sigmoid function, and $*$ is the Hadamard product."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_gAAAtscOJf-",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        embedding_dimension=100,\n",
        "        hidden_size=128, \n",
        "        n_layers=1,\n",
        "        device='cpu',\n",
        "    ):\n",
        "        super(RNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.device = device\n",
        "        \n",
        "        self.encoder = nn.Embedding(vocab_size, embedding_dimension)\n",
        "        self.rnn = nn.GRU(\n",
        "            embedding_dimension,\n",
        "            hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.randn(self.n_layers, batch_size, self.hidden_size).to(self.device)\n",
        "    \n",
        "    def forward(self, input_, hidden):\n",
        "        encoded = self.encoder(input_)\n",
        "        output, hidden = self.rnn(encoded.unsqueeze(1), hidden)\n",
        "        output = self.decoder(output.squeeze(1))\n",
        "        return output, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrC85qKz-UJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RNN(vocab_size=dataset.vocab_size, device=device).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=0.001,\n",
        ")\n",
        "scheduler = CosineAnnealingLR(optimizer, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw0OpXVz9S7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model)\n",
        "print()\n",
        "print('Trainable parameters:')\n",
        "print('\\n'.join([' * ' + x[0] for x in model.named_parameters() if x[1].requires_grad]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Edy_iSXh-UJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.train()\n",
        "train_losses = []\n",
        "for epoch in range(50):\n",
        "    progress_bar = tqdm_notebook(train_loader, leave=False)\n",
        "    losses = []\n",
        "    total = 0\n",
        "    for inputs, targets in progress_bar:\n",
        "        batch_size = inputs.size(0)\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "\n",
        "        model.zero_grad()\n",
        "        \n",
        "        loss = 0\n",
        "        for char_idx in range(inputs.size(1)):\n",
        "            output, hidden = model(inputs[:, char_idx].to(device), hidden)\n",
        "            loss += criterion(output, targets[:, char_idx].to(device))\n",
        "\n",
        "        loss.backward()\n",
        "              \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        avg_loss = loss.item() / inputs.size(1)\n",
        "        \n",
        "        progress_bar.set_description(f'Loss: {avg_loss:.3f}')\n",
        "        \n",
        "        losses.append(avg_loss)\n",
        "        total += 1\n",
        "    \n",
        "    epoch_loss = sum(losses) / total\n",
        "    train_losses.append(epoch_loss)\n",
        "        \n",
        "    tqdm.write(f'Epoch #{epoch + 1}\\tTrain Loss: {epoch_loss:.3f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsxjlCe9-UJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pretty_print(text):\n",
        "    \"\"\"Wrap text for nice printing.\"\"\"\n",
        "    to_print = ''\n",
        "    for paragraph in text.split('\\n'):\n",
        "        to_print += '\\n'.join(wrap(paragraph))\n",
        "        to_print += '\\n'\n",
        "    print(to_print)\n",
        "\n",
        "\n",
        "temperature = 0.9\n",
        "\n",
        "model.eval()\n",
        "seed = '\\n'\n",
        "text = ''\n",
        "with torch.no_grad():\n",
        "    batch_size = 1\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    last_char = dataset.char2idx[seed]\n",
        "    for _ in range(1000):\n",
        "        output, hidden = model(torch.LongTensor([last_char]).to(device), hidden)\n",
        "        \n",
        "        distribution = output.squeeze().div(temperature).exp()\n",
        "        guess = torch.multinomial(distribution, 1).item()\n",
        "        \n",
        "        last_char = guess\n",
        "        text += dataset.idx2char[guess]\n",
        "        \n",
        "pretty_print(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-RPn_4EaJzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}