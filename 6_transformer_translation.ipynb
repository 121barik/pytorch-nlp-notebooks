{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Sequence-to-Sequence Model\n",
    "\n",
    "Here we showcase a vanilla transformer model from the paper [\"Attention is all you need\"](https://arxiv.org/pdf/1706.03762.pdf) (Vaswani et al. 2017) build with both encoder and decoder layers trained on English to French translation dataset. \n",
    "\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/): A great introduction to learn the detail mechanisms of multi-head self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "\n",
    "# from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# Check out the model architecture in transformer folder\n",
    "from transformer.model import Transformer\n",
    "from transformer.batch import *\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Show better CUDA error messages\n",
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform deep learning on a GPU (so that everything runs super quick!), CUDA has to be installed and configured. Fortunately, Google Colab already has this set up, but if you want to try this on your own GPU, you can [install CUDA from here](https://developer.nvidia.com/cuda-downloads). Make sure you also [install cuDNN](https://developer.nvidia.com/cudnn) for optimized performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "\n",
    "We will download a dataset of English-to-French translations from a public Google Drive folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = 'data/english_to_french.txt'\n",
    "# if not Path(DATA_PATH).is_file():\n",
    "#     gdd.download_file_from_google_drive(\n",
    "#         file_id='1Jf7QoW2NK6_ayEXZji6DAXDSIRMvapm3',\n",
    "#         dest_path=DATA_PATH,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = '/home/jeffrey/pytorch_workshop/data/english_to_french.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40288"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = EnglishFrenchTranslations(DATA_PATH, max_vocab=1000, max_seq_len=100)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indicies of special tokens \n",
    "SRC_VOCAB = dataset.token2idx_inputs\n",
    "TRG_VOCAB = dataset.token2idx_targets\n",
    "src_pad = torch.tensor(SRC_VOCAB[dataset.padding_token]).to(device)\n",
    "src_sos = torch.tensor(SRC_VOCAB[dataset.start_of_sequence_token]).to(device)\n",
    "src_eos = torch.tensor(SRC_VOCAB[dataset.end_of_sequence_token]).to(device)\n",
    "trg_pad = torch.tensor(TRG_VOCAB[dataset.padding_token]).to(device)\n",
    "trg_sos = torch.tensor(TRG_VOCAB[dataset.start_of_sequence_token]).to(device)\n",
    "trg_eos = torch.tensor(TRG_VOCAB[dataset.end_of_sequence_token]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.99 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching - Create data generators using `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    collate_fn=lambda batch: collate(batch, src_pad, trg_pad, device),\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=1, \n",
    "    collate_fn=lambda batch: collate(batch, src_pad, trg_pad, device),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the `Transformer` model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td><img src='images/scaled_dot_product_attention.png'></td><td><img src='images/multi_head_attention.png'></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<center><img src='images/transformer.png'><center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "display(HTML(\"<table><tr><td><img src='images/scaled_dot_product_attention.png'></td><td><img src='images/multi_head_attention.png'></td></tr></table>\"))\n",
    "display(HTML(\"<center><img src='images/transformer.png'><center>\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = len(dataset.token2idx_inputs)\n",
    "trg_vocab_size = len(dataset.token2idx_targets)\n",
    "heads = 4\n",
    "N = 1\n",
    "d_model = 32 * heads\n",
    "dropout = 0.1\n",
    "\n",
    "model = Transformer(src_vocab_size, trg_vocab_size, d_model, N, heads, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sub-layers: 50\n",
      "Total number of trainable parameters: 1637864\n"
     ]
    }
   ],
   "source": [
    "param_sizes = [list(p.size()) for p in model.parameters() if p.requires_grad]\n",
    "total_params = np.sum([np.prod(size) for size in param_sizes])\n",
    "print('Number of sub-layers:', len(param_sizes))\n",
    "print('Total number of trainable parameters:', total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad)\n",
    "\n",
    "def loss_function(pred, real):\n",
    "    # Use mask to only consider non-zero inputs in the loss\n",
    "    # .ge(x) --> binary valued matrix of value > x\n",
    "    mask = real.ge(1).float().to(device)\n",
    "    loss_ = criterion(pred, real) * mask \n",
    "    return torch.mean(loss_)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training!\n",
    "\n",
    "![](images/encoder_decoder_stack.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, epochs, optimizer):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = total = 0\n",
    "        progress_bar = tqdm_notebook(train_loader, desc='Training', leave=False)\n",
    "        for inputs, targets, lengths in progress_bar:\n",
    "            # Clean old gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Create source & target sequence masks\n",
    "            src_mask, trg_mask = create_masks(\n",
    "                inputs, \n",
    "                targets[:, :-1], \n",
    "                src_pad, \n",
    "                trg_pad, \n",
    "            )\n",
    "            src_mask.to(device)\n",
    "            trg_mask.to(device)\n",
    "            \n",
    "            # Forwards pass, output: (batch_size, seq_len, target_vocab)\n",
    "            output = model(inputs, targets[:, :-1], src_mask, trg_mask)\n",
    "            # Offset target by 1 position\n",
    "            # Pred: (N, C) | y: (N,)\n",
    "            # Instead of softmax, user linear output as loss is easier to see \n",
    "            pred = output.view(-1, output.size(-1))\n",
    "            y = targets[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_function(pred, y)\n",
    "            \n",
    "            # Perform gradient descent, backwards pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Take a step in the right direction\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record metrics\n",
    "            #print('current batch loss:', round(loss.item(), 3))\n",
    "            total_loss += loss.item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        train_loss = total_loss / total\n",
    "\n",
    "        tqdm.write(f'epoch #{epoch + 1:3d}\\ttrain_loss: {train_loss:.2e}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=156, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #  1\ttrain_loss: 9.94e-03\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=156, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #  2\ttrain_loss: 6.42e-03\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "train(train_loader, model, epochs, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's translate with some test data\n",
    "\n",
    "On prediction, the model outputs probability of word on each position one by one. On each step, **beam search** is performed to keep only the top `k` sequences with highest accumlated log likelihood.\n",
    "\n",
    "![](images/beam-search.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(test_loader, src_vocab, trg_vocab, sos, pad, eos, max_seq_len, beam_size):\n",
    "    model.eval()\n",
    "    total_loss = total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, lengths in test_loader:\n",
    "            print('>', ' '.join([\n",
    "                src_vocab[int(idx)] for idx in inputs.cpu()[0].data[1:-1]\n",
    "            ]))\n",
    "            \n",
    "            # Forwards pass\n",
    "            outputs = model.predict(inputs, sos, pad, eos, max_seq_len, beam_size)\n",
    "            print(' '.join([\n",
    "                trg_vocab[int(idx)] for idx in outputs[0].data\n",
    "            ]))\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\n",
    "    test_loader, \n",
    "    dataset.idx2token_inputs, \n",
    "    dataset.idx2token_targets, \n",
    "    trg_sos, \n",
    "    trg_pad, \n",
    "    trg_eos, \n",
    "    100, \n",
    "    8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
